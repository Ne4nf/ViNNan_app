import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from .rag_chain import get_qa_chain
from .tools import process_context
import logging

load_dotenv()
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

llm = ChatOpenAI(
    api_key=OPENROUTER_API_KEY,
    base_url="https://openrouter.ai/api/v1/chat/completions",
    model_name="mistralai/mistral-small-3.2-24b-instruct:free",
    temperature=0.5
)

prompt_template = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω y t·∫ø th√¥ng minh, tr·∫£ l·ªùi c√¢u h·ªèi y t·∫ø b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch ch√≠nh x√°c v√† r√µ r√†ng. D·ª±a tr√™n ng·ªØ c·∫£nh v√† c√¢u h·ªèi, th·ª±c hi·ªán nh∆∞ sau:

1. N·∫øu y√™u c·∫ßu th√¥ng tin v·ªÅ b·ªánh, cung c·∫•p chi ti·∫øt v·ªÅ b·ªánh ƒë√≥.
2. N·∫øu l√† chu·∫©n ƒëo√°n, ph√¢n t√≠ch v√† ƒë∆∞a ra b·ªánh kh·∫£ nƒÉng cao ho·∫∑c g·ª£i √Ω n·∫øu kh√¥ng ch·∫Øc ch·∫Øn.
3. N·∫øu kh√¥ng ƒë·ªß th√¥ng tin ƒë·ªÉ x√°c ƒë·ªãnh b·ªánh, h√£y h·ªèi th√™m tri·ªáu ch·ª©ng ho·∫∑c g·ª£i √Ω c√°c nh√≥m b·ªánh li√™n quan.
4. Lu√¥n ph√¢n bi·ªát 'ƒëau ƒë·∫ßu', 's·ªët', 'ho', 'kh√≥ th·ªü', 'm·ªát' ho·∫∑c c√°c t·ª´ kh√≥a t∆∞∆°ng t·ª± l√† tri·ªáu ch·ª©ng khi ƒëi k√®m m√¥ t·∫£, kh√¥ng ph·∫£i t√™n b·ªánh.
5. N·∫øu c√≥ tri·ªáu ch·ª©ng m·ªõi, h√£y c·∫≠p nh·∫≠t danh s√°ch tri·ªáu ch·ª©ng.
6. N·∫øu trong tr∆∞·ªùng h·ª£p kh√¥ng ƒë∆∞a ra ƒë∆∞·ª£c t√™n b·ªánh c·ª• th·ªÉ (ch·ªâ ƒë∆∞a ra ƒë∆∞·ª£c c√°c nh√≥m b·ªánh li√™n quan), h√£y y√™u c·∫ßu th√™m th√¥ng tin ho·∫∑c tri·ªáu ch·ª©ng c·ª• th·ªÉ h∆°n.
7. Lu√¥n khuy·∫øn kh√≠ch ng∆∞·ªùi d√πng ƒë·∫øn b√°c sƒ© n·∫øu tri·ªáu ch·ª©ng nghi√™m tr·ªçng.

**Ng·ªØ c·∫£nh:** {context}
**C√¢u h·ªèi:** {question}
**Tri·ªáu ch·ª©ng tr∆∞·ªõc ƒë√≥ (n·∫øu c√≥):** {previous_symptoms}

**Ph·∫£n h·ªìi:**
"""

prompt = ChatPromptTemplate.from_template(prompt_template)
output_parser = StrOutputParser()

def get_llm_chain():
    qa_chain = get_qa_chain()

    def run(query, previous_symptoms=""):
        try:
            logger.info(f"üîç X·ª≠ l√Ω c√¢u h·ªèi LLM: {query}")
            context_result = process_context(query, previous_symptoms)
            processed_query = context_result["query"]
            new_symptoms = context_result["symptoms"]
            ask_confirmation = context_result.get("ask_confirmation", False)

            result = qa_chain(processed_query, previous_symptoms=new_symptoms)

            if result.get("ask_confirmation", False):
                logger.info("üîç ask_confirmation ƒë∆∞·ª£c k√≠ch ho·∫°t, tr·∫£ v·ªÅ c√¢u h·ªèi x√°c nh·∫≠n m√† kh√¥ng g·ªçi LLM.")
                return result

            context = result.get("context", "")
            if not context and result.get("possible_diseases"):
                context = f"C√°c b·ªánh c√≥ th·ªÉ li√™n quan: {', '.join(result['possible_diseases'])}"

            input_data = {
                "context": context,
                "question": query,
                "previous_symptoms": new_symptoms if new_symptoms else ""
            }

            response = prompt | llm | output_parser
            final_response = response.invoke(input_data)

            result["result"] = final_response
            return result

        except Exception as e:
            logger.error(f"‚ùå L·ªói trong LLM chain: {e}")
            return {
                "result": f"ƒê√£ x·∫£y ra l·ªói: {str(e)}",
                "disease": "",
                "possible_diseases": [],
                "context": "",
                "source_documents": [],
                "symptoms": previous_symptoms,
                "ask_confirmation": False
            }

    return run

def is_reference_to_last_disease(query):
    from .tools import detect_intent
    return detect_intent(query).get("intent") == "reference_last"